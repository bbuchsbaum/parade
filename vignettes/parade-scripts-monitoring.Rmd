---
title: "SLURM script submission and monitoring from R"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{SLURM script submission and monitoring}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

> Requires **parade ≥ 0.11.0**.

## Overview

**parade** provides a comprehensive suite of tools to submit any R script to SLURM and monitor it interactively from within R—no shell access required. This approach gives you the full power of SLURM job scheduling while maintaining the convenience of R-based workflow management.

## Key monitoring capabilities

- **Live resource monitoring**: Real-time CPU usage, memory consumption, and job status
- **Interactive dashboards**: Single job (`script_top()`) and multi-job (`jobs_top()`) monitoring
- **Log streaming**: View live output from running jobs (`script_tail()`)
- **Status checking**: Quick job state queries (`script_status()`)
- **Job management**: Cancel, wait for, and track multiple jobs
- **Error handling**: Automatic detection of failed jobs and error diagnostics

## Quick start example

```r
library(parade)
paths_init()

# Optional: Configure site defaults
slurm_defaults_set(partition="general", time="2h", cpus_per_task=16, mem=NA, persist=TRUE)
slurm_template_set("registry://templates/parade-slurm.tmpl")

# Submit a job
job <- submit_slurm("scripts/train.R", args = c("--fold", "1"))

# Quick status check
script_status(job)

# View recent log output
script_tail(job, 80)

# Launch interactive monitor
script_top(job, refresh = 2, nlog = 40)
```

## Core monitoring functions

### Single job monitoring: `script_top()`

The `script_top()` function provides a real-time, interactive dashboard for monitoring a single SLURM job:

```r
job <- submit_slurm("analysis.R")
script_top(job, refresh = 2, nlog = 30, clear = TRUE)
```

**Features displayed:**
- **Job identification**: Name, SLURM job ID, assigned node
- **Resource usage**: CPU percentage with visual progress bar, allocated vs. used CPUs
- **Memory statistics**: Average and maximum RSS (Resident Set Size), virtual memory usage
- **Timing information**: Elapsed time, CPU time used, uptime since monitoring started
- **Live log output**: Most recent log lines from the job (configurable number)
- **Status tracking**: Automatically detects when jobs complete or fail

**Parameters:**
- `refresh`: Update interval in seconds (default: 2)
- `nlog`: Number of recent log lines to display (default: 30)  
- `clear`: Whether to clear screen between updates for smoother display (default: TRUE)

### Multi-job dashboard: `jobs_top()`

Monitor multiple jobs simultaneously with a tabular overview plus detailed logs from running jobs:

```r
# Submit multiple jobs
job1 <- submit_slurm("preprocess.R", args = c("--dataset", "A"))
job2 <- submit_slurm("preprocess.R", args = c("--dataset", "B"))
job3 <- submit_slurm("model_train.R")

# Monitor all jobs together
jobs_top(list(job1, job2, job3), refresh = 3, nlog = 20)
```

**Display format:**
- **Summary line**: Count of jobs in each state (PENDING=1, RUNNING=2, etc.)
- **Job table**: Compact view with name, job ID, state, CPU%, allocated CPUs, max memory, elapsed time, node
- **Live log tail**: Recent output from the first running job

**Flexible input formats:**
```r
# List of job objects
jobs_top(list(job1, job2, job3))

# Data frame with job column
df <- data.frame(name = c("job1", "job2"), job = list(job1, job2))
jobs_top(df)

# Registry paths as strings
jobs_top(c("registry://script-abc123", "registry://script-def456"))
```

## Essential job management functions

### Status checking: `script_status()`

Get current job state without launching a full monitor:

```r
status <- script_status(job)
print(status)
# # A tibble: 1 × 5
#   pending started running  done error
#     <int>   <int>   <int> <int> <int>
#        0       0       1     0     0

# Detailed view includes full batchtools information
detailed <- script_status(job, detail = TRUE)
```

### Log viewing: `script_tail()`

Display recent log output from a job:

```r
# Show last 50 lines
script_tail(job, n = 50)

# Quick check of recent output
script_tail(job)  # Default: 200 lines
```

### Resource metrics: `script_metrics()`

Get detailed resource usage statistics:

```r
metrics <- script_metrics(job)
print(metrics)
# $job_id
# [1] "12345"
# 
# $state
# [1] "RUNNING"
# 
# $cpu_pct
# [1] 87.3
# 
# $max_rss
# [1] 1024000000  # bytes
```

### Job completion: `script_done()`

Check if a job has finished (successfully or with errors):

```r
if (script_done(job)) {
  cat("Job completed!\n")
  # Process results...
} else {
  cat("Job still running...\n")
}
```

## Advanced job management

### Waiting for completion: `script_await()`

Block execution until job completes:

```r
# Wait indefinitely
script_await(job)

# Wait with timeout (5 minutes)
script_await(job, timeout = 300)

# Custom polling interval
script_await(job, timeout = 600, poll = 30)  # Check every 30 seconds
```

### Canceling jobs: `script_cancel()`

Stop running jobs:

```r
script_cancel(job)
```

### Finding recent jobs: `script_find_latest()`

Locate recently submitted jobs when you don't have the job object:

```r
# Find 5 most recent jobs
recent <- script_find_latest(n = 5)
print(recent)

# Load a job from its registry path
job <- script_load(recent$registry[1])
```

## Practical monitoring scenarios

### Scenario 1: Long-running training job

```r
# Submit training job with generous time limit
job <- submit_slurm("train_model.R", 
                    resources = list(time = "24:00:00", mem = "32G"))

# Quick status check
if (script_status(job)$running > 0) {
  cat("Training started successfully\n")
  
  # Monitor for a few minutes, then leave it running
  script_top(job, refresh = 5, nlog = 20)
} else {
  cat("Job may be queued or failed\n")
  script_tail(job, 100)  # Check for error messages
}
```

### Scenario 2: Batch processing pipeline

```r
# Submit preprocessing jobs for multiple datasets
datasets <- c("dataset_A", "dataset_B", "dataset_C")
prep_jobs <- lapply(datasets, function(d) {
  submit_slurm("preprocess.R", args = c("--input", d))
})

# Monitor all preprocessing
jobs_top(prep_jobs, refresh = 5)

# Wait for all to complete
lapply(prep_jobs, script_await)

# Submit analysis job that depends on preprocessing
analysis_job <- submit_slurm("analyze_results.R")
script_top(analysis_job)
```

### Scenario 3: Troubleshooting failed jobs

```r
job <- submit_slurm("problematic_script.R")

# Check if job completed
if (script_done(job)) {
  status <- script_status(job)
  
  if (status$error > 0) {
    cat("Job failed! Checking logs...\n")
    
    # View full log output for debugging
    script_tail(job, n = 500)
    
    # Get log file paths for detailed analysis
    logs <- script_logs(job)
    cat("Log files:", logs$path, "\n")
  } else {
    cat("Job completed successfully\n")
  }
}
```

## Tips for efficient monitoring

### Resource optimization
- **Monitor CPU usage**: Look for jobs using less than expected CPU% - may indicate I/O bottlenecks
- **Track memory patterns**: MaxRSS shows peak memory usage; compare against requested memory
- **Watch for memory leaks**: Increasing AveRSS over time may indicate memory management issues

### Interactive monitoring best practices
- **Use appropriate refresh rates**: Fast updates (1-2s) for active debugging, slower (5-10s) for long jobs
- **Adjust log lines**: More lines (`nlog = 100`) for detailed debugging, fewer (`nlog = 10`) for overview
- **Background monitoring**: Use `clear = FALSE` when capturing output or running non-interactively

### Multi-job management
- **Group related jobs**: Monitor job families together with `jobs_top()` 
- **Stagger job submission**: Avoid overwhelming the scheduler with simultaneous submissions
- **Use descriptive names**: Job names appear in monitoring displays—make them informative

## Error handling and troubleshooting

### Common monitoring issues

**"Cannot fetch metrics" error:**
- Ensure SLURM commands (`squeue`, `sstat`, `sacct`) are available
- Check that job ID is valid and job hasn't been purged from SLURM records
- Verify SLURM permissions and cluster connectivity

**Empty log output:**
- Job may not have started writing output yet
- Check job status—may be pending in queue
- Verify output redirection in SLURM template

**Memory metrics showing NA:**
- Some metrics unavailable until job starts running
- SLURM accounting may not be enabled on cluster
- Try `script_metrics()` directly to see raw data

### Recovery strategies

**Lost job objects:**
```r
# Find recent jobs
recent <- script_find_latest(pattern = "train")
job <- script_load(recent$registry[1])
```

**Monitor jobs from different R sessions:**
```r
# Jobs persist across R sessions via registry
job_path <- "registry://script-abc123"
job <- script_load(job_path)
script_top(job)
```

This comprehensive monitoring system makes SLURM job management as convenient as local R execution while providing the scalability and resource management benefits of cluster computing.
