{"id":"manual#flow-basic","q":"How do I build a parallel dataflow from a parameter grid?","a":"Create a parameter grid with parade::grid(), wrap it in a flow with parade::flow(), add processing stages with parade::stage(), then collect results with parade::collect(). Each stage receives grid columns as arguments and returns results bound back to the grid. Stages can declare typed return schemas via the schema argument.","recipe":"library(parade)\n\ngr <- parade::grid(subject = 1:10, condition = c(\"A\", \"B\"))\nfl <- parade::flow(gr) |>\n  parade::stage(\"compute\", function(subject, condition) {\n    list(score = rnorm(1, mean = subject))\n  }) |>\n  parade::collect()","tags":["parallel","dataflow","grid","basic"],"symbols":["parade::grid","parade::flow","parade::stage","parade::collect"]}
{"id":"manual#dist-slurm","q":"How do I distribute a parade flow across SLURM nodes?","a":"Attach a SLURM distribution to your flow with parade::distribute() using parade::dist_slurm(). Set the by argument to group rows into separate SLURM jobs. Then call parade::submit() to launch jobs asynchronously and parade::deferred_collect() to gather results when they finish.","recipe":"fl <- parade::flow(gr) |>\n  parade::stage(\"fit\", function(subject) fit_model(subject)) |>\n  parade::distribute(parade::dist_slurm(\n    by = \"subject\",\n    resources = list(time = \"1:00:00\", mem = \"8G\", cpus_per_task = 4)\n  ))\n\ndeferred <- parade::submit(fl)\nresults  <- parade::deferred_collect(deferred)","tags":["slurm","hpc","distribute","submit"],"symbols":["parade::flow","parade::stage","parade::distribute","parade::dist_slurm","parade::submit","parade::deferred_collect"]}
{"id":"manual#pipeline-stages","q":"How do I chain multiple dependent processing stages in a pipeline?","a":"Use parade::pipeline() (alias for flow) and add multiple stages with parade::stage(). Declare dependencies between stages with the needs argument so downstream stages wait for upstream outputs. Each stage can access results from earlier stages by name.","recipe":"pl <- parade::pipeline(parade::grid(id = 1:5)) |>\n  parade::stage(\"preprocess\", function(id) {\n    list(data = rnorm(100))\n  }) |>\n  parade::stage(\"analyze\", function(id, preprocess) {\n    list(mean = mean(preprocess$data))\n  }, needs = \"preprocess\") |>\n  parade::collect()","tags":["pipeline","stages","dependencies","workflow"],"symbols":["parade::pipeline","parade::grid","parade::stage","parade::collect"]}
{"id":"manual#submit-script","q":"How do I submit a standalone R script to SLURM from R?","a":"Use parade::submit_slurm() to send any R script to SLURM without building a full flow. Pass the script path, optional arguments, and SLURM resource requests. The function returns a job handle you can monitor with parade::script_status() and parade::script_top().","recipe":"job <- parade::submit_slurm(\n  \"analysis/fit_model.R\",\n  args = c(\"--subject=1\", \"--output=results/\"),\n  resources = list(\n    time = \"2:00:00\",\n    mem = \"16G\",\n    cpus_per_task = 8,\n    partition = \"compute\"\n  )\n)\nparade::script_status(job)\nparade::script_top(job)","tags":["slurm","script","submit","monitoring"],"symbols":["parade::submit_slurm","parade::script_status","parade::script_top"]}
{"id":"manual#dist-local","q":"How do I run a parade flow in parallel on my local machine?","a":"Use parade::dist_local() to distribute work across local cores via the future framework. Set the by argument to control grouping and workers_within for nested parallelism. Then collect results normally. This is ideal for development and testing before moving to SLURM.","recipe":"fl <- parade::flow(parade::grid(subject = 1:20)) |>\n  parade::stage(\"process\", function(subject) {\n    list(result = expensive_computation(subject))\n  }) |>\n  parade::distribute(parade::dist_local(by = \"subject\")) |>\n  parade::collect(workers = 4)","tags":["local","parallel","future","development"],"symbols":["parade::flow","parade::grid","parade::stage","parade::distribute","parade::dist_local","parade::collect"]}
{"id":"manual#deferred-monitor","q":"How do I monitor and manage long-running SLURM jobs submitted via parade?","a":"After parade::submit(), use parade::deferred_status() for a status summary, parade::deferred_top() for a live terminal dashboard, and parade::deferred_errors() to inspect failures. Cancel with parade::deferred_cancel(). Collect partial or complete results with parade::deferred_collect().","recipe":"deferred <- parade::submit(fl)\n\n# Check status\nparade::deferred_status(deferred)\n\n# Live monitoring dashboard\nparade::deferred_top(deferred)\n\n# Inspect any errors\nparade::deferred_errors(deferred)\n\n# Collect when ready\nresults <- parade::deferred_collect(deferred)","tags":["monitoring","slurm","deferred","dashboard"],"symbols":["parade::submit","parade::deferred_status","parade::deferred_top","parade::deferred_errors","parade::deferred_collect"]}
{"id":"manual#typed-schema","q":"How do I declare typed return schemas for parade stages?","a":"Use parade::schema() (or its alias parade::returns()) to declare the expected output columns and types for a stage. Pass type constructors like dbl(), chr(), int(), and lgl() as named arguments. The schema enforces that each stage returns correctly typed results, catching mismatches early.","recipe":"fl <- parade::flow(parade::grid(x = 1:5)) |>\n  parade::stage(\"transform\", function(x) {\n    list(value = x * 2.5, label = paste0(\"item_\", x))\n  }, schema = parade::schema(value = parade::dbl(), label = parade::chr())) |>\n  parade::collect()","tags":["schema","types","validation","stage"],"symbols":["parade::schema","parade::flow","parade::grid","parade::stage","parade::collect","parade::dbl","parade::chr"]}
