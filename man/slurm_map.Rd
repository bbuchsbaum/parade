% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/slurm_map.R
\name{slurm_map}
\alias{slurm_map}
\title{Map a function or script over elements via SLURM}
\usage{
slurm_map(
  .x,
  .f,
  ...,
  .args = NULL,
  .name_by = "auto",
  .resources = NULL,
  .packages = character(),
  .write_result = NULL,
  .engine = c("slurm", "local"),
  .progress = FALSE,
  .options = NULL,
  .error_policy = NULL,
  .packed = FALSE,
  .workers_per_node = NULL,
  .chunk_size = NULL,
  .parallel_backend = c("callr", "auto", "multicore", "multisession")
)
}
\arguments{
\item{.x}{Vector or list to map over}

\item{.f}{Function, formula, or script path to apply to each element}

\item{...}{Additional arguments passed to the function or script}

\item{.args}{Named list of additional arguments (alternative to ...)}

\item{.name_by}{Naming strategy: "auto", "index", "stem", "digest", or a function}

\item{.resources}{Resource specification (profile name, profile object, list, or NULL)}

\item{.packages}{Character vector of packages to load (for functions)}

\item{.write_result}{Path template for saving results (supports macros)}

\item{.engine}{Execution engine: "slurm" (default) or "local"}

\item{.progress}{Show progress bar}

\item{.options}{Flow control options (e.g., wave_policy() or concurrency_limit())}

\item{.error_policy}{Error handling policy for job failures}

\item{.packed}{Logical; if TRUE, pack multiple tasks into single SLURM jobs for
efficient node utilization (default: FALSE)}

\item{.workers_per_node}{Integer; number of parallel workers per node when packed
(defaults to resources$cpus_per_task if present, else 1)}

\item{.chunk_size}{Integer; number of tasks per packed job (defaults to .workers_per_node)}

\item{.parallel_backend}{Backend for within-node parallelism when \code{.packed = TRUE}.
One of: "callr", "multicore", "multisession", or "auto". Ignored when
\code{.packed = FALSE}. Defaults to "callr" for strong isolation.}
}
\value{
A \code{parade_jobset} object containing all submitted jobs
}
\description{
Submits multiple SLURM jobs by mapping a function or script over a vector
or list. Automatically dispatches to \code{slurm_call} for functions or
\code{submit_slurm} for scripts.
}
\details{
When \code{.f} is a function or formula (e.g., \code{~ .x + 1}), each element
of \code{.x} is passed as the first argument to the function. When \code{.f}
is a character string path to a script, it's treated as a script submission
with appropriate argument conversion.

The \code{.name_by} parameter controls job naming:
\itemize{
\item "auto": Automatic naming based on context
\item "index": Use numeric index (job-1, job-2, etc.)
\item "stem": Extract stem from file paths in .x
\item "digest": Use content hash
\item function: Custom naming function receiving element and index
}

\strong{Packed Execution for HPC Efficiency:}

Use \code{.packed = TRUE} to pack multiple tasks into single SLURM jobs for better
node utilization on HPC systems. This is critical when admins expect full-node
allocations:
\itemize{
\item \strong{Standard mode} (\code{.packed = FALSE}): 1000 files → 1000 SLURM jobs → likely 1000 nodes
\item \strong{Packed mode} (\code{.packed = TRUE}, \code{.workers_per_node = 20}): 1000 files →
50 SLURM jobs → 50 nodes, each using 20 cores
}

Packed mode automatically:
\itemize{
\item Chunks inputs into batches
\item Requests appropriate \code{cpus_per_task}
\item Runs tasks in parallel per node using the selected backend (\code{.parallel_backend}):
"callr" (default, most isolated), "multicore" (HPC Linux), or "multisession"
\item Works with flow control via \code{.options} (e.g., \code{max_in_flight()})
\item Preserves element-level naming and result writing with \code{{stem}}, \code{{run}} macros
}
}
\examples{
# Local execution example (no SLURM required)
local_jobs <- slurm_map(1:3, ~ .x^2, .engine = "local")
results <- collect(local_jobs)

\donttest{
# Note: The following examples require a SLURM cluster environment
if (Sys.which("squeue") != "") {
  # Map a function over files
  files <- c("data1.csv", "data2.csv")
  process_data <- function() identity  # stub for example
  jobs <- slurm_map(files, ~ read.csv(.x) |> process_data(),
                    .name_by = "stem",
                    .write_result = "results/{stem}.rds")

  # Map a script with CLI arguments
  jobs <- slurm_map(files, "scripts/process.R",
                    .args = args_cli(verbose = TRUE))

  # Use formula notation with SLURM
  numbers <- 1:10
  jobs <- slurm_map(numbers, ~ .x^2 + .x,
                    .name_by = "index")

  # PACKED EXECUTION: Process 1000 files using 20 cores per node
  # This submits ~50 jobs instead of 1000, making HPC admins happy
  files <- glob("data/*.csv")
  jobs <- slurm_map(
    files,
    ~ read.csv(.x)[1:5, ],
    .name_by = "stem",
    .write_result = path$artifacts("results/{run}/{stem}.rds"),
    .packed = TRUE,
    .workers_per_node = 20,
    .resources = list(cpus_per_task = 20, mem = "64G", time = "4h")
  )
  # Track progress and collect element-level results
  results <- jobs |> progress() |> collect()  # Returns 1000 results

  # Wait for all jobs and collect results
  results <- jobs |> await() |> collect()
}
}

}
