% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/slurm_cluster.R
\name{slurm_cluster_plan}
\alias{slurm_cluster_plan}
\title{Plan packed chunking for a fixed-size SLURM cluster}
\usage{
slurm_cluster_plan(
  n_tasks,
  nodes,
  cpus_per_node,
  oversubscribe = 2L,
  nodes_per_job = 1L
)
}
\arguments{
\item{n_tasks}{Integer; number of tasks you will map over.}

\item{nodes}{Integer; number of nodes you expect SLURM to run concurrently
(e.g., 10).}

\item{cpus_per_node}{Integer; cores per node (e.g., 196).}

\item{oversubscribe}{Integer; how many jobs to queue per concurrently usable
node. Using \code{oversubscribe > 1} helps mitigate stragglers for heterogeneous
task durations (SLURM will start the next job as nodes free up).}

\item{nodes_per_job}{Integer; number of nodes each packed job requests.
Defaults to 1.}
}
\value{
A named list with:
\itemize{
\item \code{workers_per_node}: recommended \code{.workers_per_node}
\item \code{target_jobs}: recommended \code{.target_jobs}
\item \code{chunk_size}: recommended \code{.chunk_size} (if you want to pin it)
\item \code{resources}: resource list you can pass as \code{.resources}
}
}
\description{
Parade cannot (yet) provide a true "SLURM pool" that behaves like a single
machine with \code{nodes * cpus_per_node} cores. However, you can get close today
by using packed jobs (\code{slurm_map(.packed = TRUE, ...)}) and choosing a chunk
size that yields a reasonable number of SLURM jobs.
}
\details{
This helper computes a sensible packed configuration for a fixed cluster size
and a known task count.
}
